# ðŸ¤– LLM Code Audit - Comprehensive Documentation

A smart auditing tool that helps developers compare and evaluate AI-generated code from multiple LLMs like ChatGPT, Gemini, DeepSeek, LLaMA, and Mistral.

---

##  Table of Contents
1. [Introduction](#introduction)
2. [Problem Statement](#problem-statement)
3. [Solution Overview](#solution-overview)
4. [Features](#features)
5. [Technical Architecture](#technical-architecture)
6. [File Structure](#file-structure)
7. [Detailed Page Descriptions](#detailed-page-descriptions)
8. [Metrics Explanation](#metrics-explanation)
9. [Conclusion](#conclusion)

---

##  Introduction

![Homepage](./screenshots/homepage.png)
![Model selection](./screenshots/modelselection.png)

**LLM Code Audit** helps developers analyze and compare code generated by multiple AI tools to identify the best-performing and most maintainable solution.

---

##  Problem Statement

###  The Problem
- AI-generated code is often unoptimized or poorly structured.
- Manually comparing outputs is tedious and error-prone.
- Lack of tools that provide actionable feedback and ranking across LLMs.



###  Why Current Tools Fall Short
- Tools like SonarQube evaluate only post-development.
- No side-by-side LLM comparisons.
- No automatic quality metrics or recommendations.

---

##  Solution Overview

![Dashboard](./screenshots/dashboard1.png)
![Dashboard](./screenshots/dashboard2.png)
![Dashboard](./screenshots/dashboard3.png)
![Dashboard](./screenshots/dashboard4.png)

**LLM Code Audit** provides:
-  Multi-LLM integration
-  AI code comparison
-  20+ quality metrics
-  Scoring and suggestions

---

##  Features

### 1.  Multi-LLM Code Generation
- One prompt â†’ multiple AI outputs
- Side-by-side comparison
- Add user code for evaluation

### 2.  Advanced Code Analysis
- Detects code issues
- Auto-scores and recommends best output

### 3.  Smart Metrics System
- ACI (Complexity)
- AMR (Maintainability Risk)
- ARS (Readability)
- ADR (Dependency Risk)
- ARF (Redundancy)

### 4.  History Tracking
![History Page](./screenshots/history.png)
- Stores prompts and outputs
- Tracks best LLM per prompt
- Allows re-analysis

### 5.  User-Friendly Interface
- Minimal, modern UI
- Interactive visualizations
- Fully responsive

---

##  Technical Architecture

### Frontend
- React.js with Vite
- Modular components
- Styled using custom CSS

### Backend
- Node.js
- Firebase Authentication
- Firestore for prompt/output storage

### AI Integrations
- ChatGPT
- Gemini
- DeepSeek
- LLaMA
- Mistral



---

##  File Structure

```txt
node_modules/
public/
src/
â”œâ”€â”€ api/
â”œâ”€â”€ assets/
â”œâ”€â”€ backend/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ AuthModal.jsx
â”‚   â”œâ”€â”€ Header.jsx
â”‚   â”œâ”€â”€ HistoryItem.jsx
â”‚   â”œâ”€â”€ QueryInput.jsx
â”‚   â””â”€â”€ ResultsDisplay.jsx
â”œâ”€â”€ hooks/
â”œâ”€â”€ pages/
â”‚   â”œâ”€â”€ AboutPage.jsx
â”‚   â”œâ”€â”€ AuthPage.jsx
â”‚   â”œâ”€â”€ ComparisonDetail.jsx
â”‚   â”œâ”€â”€ Dashboard.jsx
â”‚   â”œâ”€â”€ HistoryPage.jsx
â”‚   â””â”€â”€ HomePage.jsx
â”œâ”€â”€ styles/
â”œâ”€â”€ App.css
â”œâ”€â”€ App.jsx
â”œâ”€â”€ firebaseConfig.js
â”œâ”€â”€ firestoreService.js
â”œâ”€â”€ index.css
â”œâ”€â”€ main.jsx
.env
.gitignore
eslint.config.js
index.html
package-lock.json
package.json
vite.config.js


---

##  Detailed Page Descriptions

###  Home Page
- Enter prompt  
- Choose LLMs  
- Add custom code  
- Click "Generate" for outputs  

###  Dashboard Page
- View AI-generated code  
- Start code evaluation

###  Results Page
- Scores and charts for each output  
- Final recommendation  

###  History Page
- View and revisit previous prompts  
- Compare LLM performance over time  

### â„¹ About Page
- Project vision  
- Team details  
- Real-world applications  

---

##  Metrics Explanation

###  AI Complexity Index (ACI)

```mathematica
ACI = (0.5 Ã— Cognitive Complexity) + (0.3 Ã— Method Length) + (0.2 Ã— Nesting Level)



##  Conclusion

**LLM Code Audit** empowers developers to take full control over AI-generated code, ensuring quality, maintainability, and clarity before integrating into production environments.

By offering clear comparisons, actionable insights, and in-depth metrics, this tool bridges the gap between rapid AI code generation and robust software engineering standards.

>  Ready to reduce tech debt and make AI code work for you?  
>  Try **LLM Code Audit** now!

